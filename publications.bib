@incollection{cavdir2021reactive,
  title={Reactive Video: Movement Sonification as Auditory Feedback for Supporting Physical Activity},
  author={Cavdir, Doga and Clarke, Christopher and Chiu, Patrick and Denoue, Laurent and Kimber, Don},
  booktitle={New Interfaces for Musical Expression (NIME) 2021},
  year={2021}
}

@inproceedings{10.1145/3411764.3445205,
author = {R\"{o}ddiger, Tobias and Clarke, Christopher and Wolffram, Daniel and Budde, Matthias and Beigl, Michael},
title = {EarRumble: Discreet Hands- and Eyes-Free Input by Voluntary Tensor Tympani Muscle Contraction},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445205},
doi = {10.1145/3411764.3445205},
abstract = {We explore how discreet input can be provided using the tensor tympani - a small muscle
in the middle ear that some people can voluntarily contract to induce a dull rumbling
sound. We investigate the prevalence and ability to control the muscle through an
online questionnaire (N=192) in which 43.2% of respondents reported the ability to
“ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can
be used to detect voluntary tensor tympani contraction in the sealed ear canal. This
data was used to train a classifier based on three simple ear rumble “gestures” which
achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction,
grounded in three manual, dual-task application scenarios (N=8). This highlights the
applicability of EarRumble as a low-effort and discreet eyes- and hands-free interaction
technique that users found “magical” and “almost telepathic”. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {743},
numpages = {14},
keywords = {discreet interaction, in-ear barometry, hearables, earables, tensor tympani muscle, subtle gestures},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3448017.3457381,
author = {Ramirez Gomez, Argenis Ramirez and Clarke, Christopher and Sidenmark, Ludwig and Gellersen, Hans},
title = {Gaze+Hold: Eyes-Only Direct Manipulation with Continuous Gaze Modulated by Closure of One Eye},
year = {2021},
isbn = {9781450383448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448017.3457381},
doi = {10.1145/3448017.3457381},
abstract = { The eyes are coupled in their gaze function and therefore usually treated as a single
input channel, limiting the range of interactions. However, people are able to open
and close one eye while still gazing with the other. We introduce Gaze+Hold as an
eyes-only technique that builds on this ability to leverage the eyes as separate input
channels, with one eye modulating the state of interaction while the other provides
continuous input. Gaze+Hold enables direct manipulation beyond pointing which we explore
through the design of Gaze+Hold techniques for a range of user interface tasks. In
a user study, we evaluated performance, usability and user’s spontaneous choice of
eye for modulation of input. The results show that users are effective with Gaze+Hold.
The choice of dominant versus non-dominant eye had no effect on performance, perceived
usability and workload. This is significant for the utility of Gaze+Hold as it affords
flexibility for mapping of either eye in different configurations.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {10},
numpages = {12},
keywords = {Winks, Eye Tracking, Design, Gaze Pointing, Direct Manipulation, Closing eyelids, Gaze Interaction},
location = {Virtual Event, Germany},
series = {ETRA '21 Full Papers}
}

@inproceedings{10.1145/3379337.3415591,
author = {Clarke, Christopher and Cavdir, Doga and Chiu, Patrick and Denoue, Laurent and Kimber, Don},
title = {Reactive Video: Adaptive Video Playback Based on User Motion for Supporting Physical Activity},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415591},
doi = {10.1145/3379337.3415591},
abstract = {Videos are a convenient platform to begin, maintain, or improve a fitness program
or physical activity. Traditional video systems allow users to manipulate videos through
specific user interface actions such as button clicks or mouse drags, but have no
model of what the user is doing and are unable to adapt in useful ways. We present
adaptive video playback, which seamlessly synchronises video playback with the user's
movements, building upon the principle of direct manipulation video navigation. We
implement adaptive video playback in Reactive Video, a vision-based system which supports
users learning or practising a physical skill. The use of pre-existing videos removes
the need to create bespoke content or specially authored videos, and the system can
provide real-time guidance and feedback to better support users when learning new
movements. Adaptive video playback using a discrete Bayes and particle filter are
evaluated on a data set collected of participants performing tai chi and radio exercises.
Results show that both approaches can accurately adapt to the user's movements, however
reversing playback can be problematic.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {196–208},
numpages = {13},
keywords = {physical activity, direct manipulation, full body, probabilistic},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3428361.3428470,
author = {Clarke, Christopher and Ehrich, Peter and Gellersen, Hans},
title = {Motion Coupling of Earable Devices in Camera View},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428470},
doi = {10.1145/3428361.3428470},
abstract = { Earables, earphones augmented with inertial sensors and real-time data accessibility,
provide the opportunity for private audio channels in public settings. One of the
main challenges of achieving this goal is to correctly associate which device belongs
to which user without prior information. In this paper, we explore how motion of an
earable, as measured by the on-board accelerometer, can be correlated against detected
faces from a webcam to accurately match which user is wearing the device. We conduct
a data collection and explore which type of user movement can be accurately detected
using this approach, and investigate how varying the speed of the movement affects
detection rates. Our results show that the approach achieves greater detection results
for faster movements, and that it can differentiate the same movement across different
participants with a detection rate of 86%, increasing to 92% when differentiating
a movement against others.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {13–17},
numpages = {5},
keywords = {Spontaneous device association, earable, motion coupling.},
location = {Essen, Germany},
series = {MUM 2020}
}

@inproceedings{10.1145/3379155.3391312,
author = {Sidenmark, Ludwig and Mardanbegi, Diako and Gomez, Argenis Ramirez and Clarke, Christopher and Gellersen, Hans},
title = {BimodalGaze: Seamlessly Refined Pointing with Gaze and Filtered Gestural Head Movement},
year = {2020},
isbn = {9781450371339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379155.3391312},
doi = {10.1145/3379155.3391312},
abstract = {Eye gaze is a fast and ergonomic modality for pointing but limited in precision and
accuracy. In this work, we introduce BimodalGaze, a novel technique for seamless head-based
refinement of a gaze cursor. The technique leverages eye-head coordination insights
to separate natural from gestural head movement. This allows users to quickly shift
their gaze to targets over larger fields of view with naturally combined eye-head
movement, and to refine the cursor position with gestural head movement. In contrast
to an existing baseline, head refinement is invoked automatically, and only if a target
is not already acquired by the initial gaze shift. Study results show that users reliably
achieve fine-grained target selection, but we observed a higher rate of initial selection
errors affecting overall performance. An in-depth analysis of user performance provides
insight into the classification of natural versus gestural head movement, for improvement
of BimodalGaze and other potential applications. },
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {8},
numpages = {9},
keywords = {Gaze interaction, Eye tracking, Refinement, Virtual reality, Eye-head coordination},
location = {Stuttgart, Germany},
series = {ETRA '20 Full Papers}
}

@inproceedings{10.1145/3313831.3376438,
author = {Sidenmark, Ludwig and Clarke, Christopher and Zhang, Xuesong and Phu, Jenny and Gellersen, Hans},
title = {Outline Pursuits: Gaze-Assisted Selection of Occluded Objects in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376438},
doi = {10.1145/3313831.3376438},
abstract = {In 3D environments, objects can be difficult to select when they overlap, as this
affects available target area and increases selection ambiguity. We introduce Outline
Pursuits which extends a primary pointing modality for gaze-assisted selection of
occluded objects. Candidate targets within a pointing cone are presented with an outline
that is traversed by a moving stimulus. This affords completion of the selection by
gaze attention to the intended target's outline motion, detected by matching the user's
smooth pursuit eye movement. We demonstrate two techniques implemented based on the
concept, one with a controller as the primary pointer, and one in which Outline Pursuits
are combined with head pointing for hands-free selection. Compared with conventional
raycasting, the techniques require less movement for selection as users do not need
to reposition themselves for a better line of sight, and selection time and accuracy
are less affected when targets become highly occluded.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {eye tracking, occlusion, smooth pursuits, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@book{clarke2020dynamic,
  title={Dynamic motion coupling of body movement for input control},
  author={Clarke, Christopher},
  year={2020},
  publisher={Lancaster University (United Kingdom)}
}

@inproceedings{10.1145/3314111.3319822,
author = {Mardanbegi, Diako and Clarke, Christopher and Gellersen, Hans},
title = {Monocular Gaze Depth Estimation Using the Vestibulo-Ocular Reflex},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314111.3319822},
doi = {10.1145/3314111.3319822},
abstract = {Gaze depth estimation presents a challenge for eye tracking in 3D. This work investigates
a novel approach to the problem based on eye movement mediated by the vestibulo-ocular
reflex (VOR). VOR stabilises gaze on a target during head movement, with eye movement
in the opposite direction, and the VOR gain increases the closer the fixated target
is to the viewer. We present a theoretical analysis of the relationship between VOR
gain and depth which we investigate with empirical data collected in a user study
(N=10). We show that VOR gain can be captured using pupil centres, and propose and
evaluate a practical method for gaze depth estimation based on a generic function
of VOR gain and two-point depth calibration. The results show that VOR gain is comparable
with vergence in capturing depth while only requiring one eye, and provide insight
into open challenges in harnessing VOR gain as a robust measure.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications},
articleno = {20},
numpages = {9},
keywords = {eye movement, eye tracking, 3D gaze estimation, fixation depth, VOR, gaze depth estimation},
location = {Denver, Colorado},
series = {ETRA '19}
}

@inproceedings{10.1145/3126594.3126626,
author = {Clarke, Christopher and Gellersen, Hans},
title = {MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing},
year = {2017},
isbn = {9781450349819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126594.3126626},
doi = {10.1145/3126594.3126626},
abstract = {Pointing is a fundamental interaction technique where user movement is translated
to spatial input on a display. Conventionally, this is based on a rigid configuration
of a display coupled with a pointing device that determines the types of movement
that can be sensed, and the specific ways users can affect pointer input. Spontaneous
spatial coupling is a novel input technique that instead allows any body movement,
or movement of tangible objects, to be appropriated for touchless pointing on an ad
hoc basis. Pointer acquisition is facilitated by the display presenting graphical
objects in motion, to which users can synchronise to define a temporary spatial coupling
with the body part or tangible object they used in the process. The technique can
be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer
vision-based implementation of the technique that requires only a webcam. We explore
the design space of spontaneous spatial coupling, demonstrate the versatility of the
technique with application examples, and evaluate MatchPoint performance using a multi-directional
pointing task.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {179–192},
numpages = {14},
keywords = {vison-based interfaces, computer vision, gesture input, pointing, input techniques, touchless input, user input, bodily interaction, motion-matching},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17}
}

@article{10.1145/3130910,
author = {Clarke, Christopher and Bellino, Alessio and Esteves, Augusto and Gellersen, Hans},
title = {Remote Control by Body Movement in Synchrony with Orbiting Widgets: An Evaluation of TraceMatch},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130910},
doi = {10.1145/3130910},
abstract = {In this work we consider how users can use body movement for remote control with minimal
effort and maximum flexibility. TraceMatch is a novel technique where the interface
displays available controls as circular widgets with orbiting targets, and where users
can trigger a control by mimicking the displayed motion. The technique uses computer
vision to detect circular motion as a uniform type of input, but is highly appropriable
as users can produce matching motion with any part of their body. We present three
studies that investigate input performance with different parts of the body, user
preferences, and spontaneous choice of movements for input in realistic application
scenarios. The results show that users can provide effective input with their head,
hands and while holding objects, that multiple controls can be effectively distinguished
by the difference in presented phase and direction of movement, and that users choose
and switch modes of input seamlessly.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {45},
numpages = {22},
keywords = {Gesture input, Movement correlation, Motion matching, Computer vision, Input techniques, Remote control, User input, Motion correlation, Path mimicry, User evaluation, Vision-based interfaces}
}

@article{10.1145/3064937,
author = {Velloso, Eduardo and Carter, Marcus and Newn, Joshua and Esteves, Augusto and Clarke, Christopher and Gellersen, Hans},
title = {Motion Correlation: Selecting Objects by Matching Their Movement},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3064937},
doi = {10.1145/3064937},
abstract = {Selection is a canonical task in user interfaces, commonly supported by presenting
objects for acquisition by pointing. In this article, we consider motion correlation
as an alternative for selection. The principle is to represent available objects by
motion in the interface, have users identify a target by mimicking its specific motion,
and use the correlation between the system’s output with the user’s input to determine
the selection. The resulting interaction has compelling properties, as users are guided
by motion feedback, and only need to copy a presented motion. Motion correlation has
been explored in earlier work but only recently begun to feature in holistic interface
designs. We provide a first comprehensive review of the principle, and present an
analysis of five previously published works, in which motion correlation underpinned
the design of novel gaze and gesture interfaces for diverse application contexts.
We derive guidelines for motion correlation algorithms, motion feedback, choice of
modalities, overall design of motion correlation interfaces, and identify opportunities
and challenges identified for future research and design.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {22},
numpages = {35},
keywords = {motion tracking, natural user interfaces, gesture interfaces, gaze interaction, Motion correlation, interaction techniques, eye tracking}
}

@article{angelov2017aurora,
  title={AURORA: autonomous real-time on-board video analytics},
  author={Angelov, Plamen and Sadeghi Tehran, Pouria and Clarke, Christopher},
  journal={Neural Computing and Applications},
  volume={28},
  number={5},
  pages={855--865},
  year={2017}
}

@inproceedings{10.1145/2971648.2971714,
author = {Clarke, Christopher and Bellino, Alessio and Esteves, Augusto and Velloso, Eduardo and Gellersen, Hans},
title = {TraceMatch: A Computer Vision Technique for User Input by Tracing of Animated Controls},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971714},
doi = {10.1145/2971648.2971714},
abstract = {Recent works have explored the concept of movement correlation interfaces, in which
moving objects can be selected by matching the movement of the input device to that
of the desired object. Previous techniques relied on a single modality (e.g. gaze
or mid-air gestures) and specific hardware to issue commands. TraceMatch is a computer
vision technique that enables input by movement correlation while abstracting from
any particular input modality. The technique relies only on a conventional webcam
to enable users to produce matching gestures with any given body parts, even whilst
holding objects. We describe an implementation of the technique for acquisition of
orbiting targets, evaluate algorithm performance for different target sizes and frequencies,
and demonstrate use of the technique for remote control of graphical as well as physical
objects with different body parts.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {298–303},
numpages = {6},
keywords = {vision-based interfaces, path mimicry, remote control, input techniques, motion matching, computer vision, ubiquitous computing, gesture input, user input},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/2702613.2732842,
author = {Sas, Corina and Challioner, Scott and Clarke, Christopher and Wilson, Ross and Coman, Alina and Clinch, Sarah and Harding, Mike and Davies, Nigel},
title = {Self-Defining Memory Cues: Creative Expression and Emotional Meaning},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732842},
doi = {10.1145/2702613.2732842},
abstract = {This paper explores how people generate cues for capturing personal meaningful daily
events, which can be used for later recall. Such understanding can be explored to
inform the design and development of personal informatics systems, aimed to support
reflection and increased self-awareness. We describe a diary study with six participants
and discuss initial findings showing the qualities of daily meaningful events, the
value of different types of cues and their distinct contents for supporting episodic
recall.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2013–2018},
numpages = {6},
keywords = {emotions, meaningful daily events, doodling, creativity, episodic memory recall, self-generated cues},
location = {Seoul, Republic of Korea},
series = {CHI EA '15}
}

@inproceedings{sadeghi2014real,
  title={A real-time approach for autonomous detection and tracking of moving objects from UAV},
  author={Sadeghi-Tehran, Pouria and Clarke, Christopher and Angelov, Plamen},
  booktitle={2014 IEEE Symposium on Evolving and Autonomous Learning Systems (EALS)},
  pages={43--49},
  year={2014},
  organization={IEEE}
}

@article{clarke2014sariva,
  title={Sariva: Smartphone app for real-time intelligent video analytics},
  author={Clarke, Christopher and Angelov, Plamen},
  journal={Journal of Automation, Mobile Robotics and Intelligent Systems},
  pages={15--19},
  year={2014}
}
